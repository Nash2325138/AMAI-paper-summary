# Discovering Discrete Latent Topics with Neural Variational Inference

Intro:
- Topic models: probabilistic generative models of documents.
- Traditional topic models use closed-form derivations for updating the models, but it's not scalable when the expressiveness grows
- This paper use NN to model **parameterisable** distributions over topics.
- This paper also propose to use RNN to discover **variable** number of topics.

---

## Parameterising Topic Distributions

Below are the formulation comparison between the traditional method LDA and the new proposed method.

In the aspect of **generative process**:

LDA: <img src="https://i.imgur.com/STt6YSB.png" width=300>

Proposed method: <img src="https://i.imgur.com/oqagOLr.png" width=300>

The difference: the **latent variables** θ_d is now sampled from G, which is composed of a **neural network**
conditioned on an isotropic Gaussian. (much like the decoder part of a VAE structure)

Tn the aspect of **marginal likelihood for a document in collection D**:

LDA: <img src="https://i.imgur.com/n2uoMwi.png" width=450>

Proposed method: <img src="https://i.imgur.com/ofDUxt3.png" width=450>

In (2), it parameterises the latent variable θ by a neural network conditioned on a draw
from a Gaussian distribution.

A note here, the multinomial document topic distributions is derived by passing a Gaussian random vector through
a softmax function:
<img src="https://i.imgur.com/dlHCC0N.png" width=150>

---

## Stick Breaking Construction

<img src="https://i.imgur.com/DJ0OZXL.png" width=400>

In the Dirichlet constructive process, this paper propose a Recurrent Stick Breaking Construction to
compare with a commonly used alternatives: Gaussian Stick Breaking Construction

### Gaussian Stick Breaking Construction
conditioned on a Gaussian sample x ∈ R^H, the breaking proportions
η ∈ R^(K−1) are generated by applying the sigmoid function
<img src="https://i.imgur.com/uooCAWQ.png" width=150>

### Recurrent Stick Breaking Construction
the portions are instead generated by RNN as illustrated below:

<img src="https://i.imgur.com/oxf9aqh.png" width=350>

<img src="https://i.imgur.com/gTs62Lt.png" width=200>

And therefore, the Dirichlet constructive process becomes: 
<img src="https://i.imgur.com/OhNwEAL.png" width=150>

The main advantage to use RNN here is that it **captures an unbounded number of breaks** with a **finite parameters**.

## Nueral models
This paper introduces 2 kinds of neural topic models and corresponding inference methods.

### Neural Topic Models
<img src="https://i.imgur.com/WfwZ6Sq.png" width=400>

as illustrated, a variational lower bound for the document log-likelihood according to Equation (2) can be derived:
<img src="https://i.imgur.com/CyvYy7K.png" width=500>

Followed by a series of integration about the KL term and the latent variable, it can be simplified as:
<img src="https://i.imgur.com/vhkoLZB.png" width=450>

Note that the prior θ here can be generated by Gaussian Softmax, Gaussian Stick Breaking, or Recurrent Stick Breaking constructions.


### Recurrent Neural Topic Models
In addition to the RNN_SB that generates the topic proportions θ ∈ R^∞ for each document,
it's neccesary to **introduce another RNN to produce topics dynamically** (in terms of the number of active topics).

<img src="http://i.imgur.com/Jo4Xg0y.png" width=400>

<img src="https://i.imgur.com/Ib4JITN.png" width=200>

The variational lower bound for a document d:
<img src="https://i.imgur.com/p4QkkZO.png" width=300>

## Experiments

### Perplexities of the different topic models
<img src="http://i.imgur.com/jahtLpB.png" width=400>

The Gaussian softmax construction (GSM) achieves the lowest perplexity in most cases, while GSM, GSB and RSB models are all better than the benchmark models.

### Perplexities of the different document models
<img src="http://i.imgur.com/Hs0Du6X.png" width=400>

We can see that by using an implicit topic distribution, the generalisation abilities of the 3 models are all improved and performance better than benchmarks.

### Others
Test different topic numbers:

<img src="http://i.imgur.com/RGc33Qi.png" width=350>

Test the convergence behavior:

<img src="http://i.imgur.com/j7LynvO.png" width=350>
