# Discovering Discrete Latent Topics with Neural Variational Inference

Intro:
- Topic models: probabilistic generative models of documents.
- Traditional topic models use closed-form derivations for updating the models, but it's not scalable when the expressiveness grows
- This paper use NN to model **parameterisable** distributions over topics.
- This paper also propose to use RNN to discover **variable** number of topics.

---

## Parameterising Topic Distributions

Below are the formulation comparison between the traditional method LDA and the new proposed method.

In the aspect of **generative process**:

LDA: <img src="https://i.imgur.com/STt6YSB.png" width=300>

Proposed method: <img src="https://i.imgur.com/oqagOLr.png" width=300>

The difference: the **latent variables** θ_d is now sampled from G, which is composed of a **neural network**
conditioned on an isotropic Gaussian. (much like the decoder part of a VAE structure)

Tn the aspect of **marginal likelihood for a document in collection D**:

LDA: <img src="https://i.imgur.com/n2uoMwi.png" width=450>

Proposed method: <img src="https://i.imgur.com/ofDUxt3.png" width=450>

In (2), it parameterises the latent variable θ by a neural network conditioned on a draw
from a Gaussian distribution.

A note here, the multinomial document topic distributions is derived by passing a Gaussian random vector through
a softmax function:
<img src="https://i.imgur.com/dlHCC0N.png" width=150>

---

## Stick Breaking Construction

<img src="https://i.imgur.com/DJ0OZXL.png" width=400>

In the Dirichlet constructive process, this paper propose a **Recurrent Stick Breaking Construction** to
compare with a commonly used alternatives: **Gaussian Stick Breaking Construction**

**Gaussian Stick Breaking Construction**:
conditioned on a Gaussian sample x ∈ R^H, the breaking proportions
η ∈ R^(K−1) are generated by applying the sigmoid function
<img src="https://i.imgur.com/uooCAWQ.png" width=150>

**Recurrent Stick Breaking Construction**:
the portions are instead generated by RNN as illustrated below:

<img src="https://i.imgur.com/oxf9aqh.png" width=350>

<img src="https://i.imgur.com/gTs62Lt.png" width=200>

And therefore, the Dirichlet constructive process becomes: 
<img src="https://i.imgur.com/OhNwEAL.png" width=150>
