# [Paper reading] Learning From Noisy Large-Scale Datasets With Minimal Supervision
Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie, CVPR 2017

## Introduction
Deep learning methods often require a large dataset to achieve good performance. However, datasets of this kind often require a huge amount of human annotations, which makes the collection process pretty expensive.

On the other hand, datasets with **unlabeled data or noisy annotations** are more common in reality, so lots of works related to unsupervied, semi-supervided, or self-supervided learning are studied recently to utilize the rich information under these datasets.


## Contribution

This work introduce an new approach to leverage datasets with a conjunction of **a small set of clean annotations and a large set of noisy annotations**.

One common approach is to pretrain a network on the noise data and then finetune the network with the clean dataset. This paper demonstrates that the proposed method significantly improved the performance over traditional fine-tuning approach and provide the first benchmark on the Open Image dataset.

## Problem Setting 
The method is experimented on a classic image classification problem. All of the image in the dataset has a noisy label $y$, but only a small portion of image have human-verified label $v$.

## Method

### Architecture
![](https://i.imgur.com/n6dmBGo.png)
The model architecture can be divided into 3 parts:
1. Convolutional Network: the image feature extractor
2. Label Cleaning Network: a label noise reducer to transform noisy labels to clean labels
3. Image Classifier: one fully connected layer followed by a sigmoid function

### General Concept
1. Use CNN to extract image features
2. Feed the image features along with noisy labels to "Label Cleaning Network" to predict a cleaned label $c$ and compute loss $L_{\text{clean}}$
3. Feed the image features to "Image Classifier" to predict a label $p$ and compute loss $L_{\text{classify}}$
4. Gradient decent

For $L_{\text{clean}}$, only the data samples **with verified labels** are used.

For $L_{\text{classify}}$, it's supervised by **verified labels** $v$ when available (with subscript $i$), otherwise the **cleaned label** $c$ will be used (with subscript $j$).

### Losses Design
<p align="center">
  <img src="https://i.imgur.com/5Apg0Fy.png" width="200">
</p>

The label cleaning loss is defined by L1 loss instead of L2, which is to prevent the model generate a "smooth" label.
<br>

<p align="center">
  <img src="https://i.imgur.com/dVpqg0p.png" width="420">
</p>

The classification loss is defined by cross-entropy loss. One thing to notice is that the **cleaned label** is seen as constants, so the **classification loss** does not backpropagation to the **Label Cleaning Network**. The reason is because the examples without verified labels donimate the training set, the network will have a trivial optimal solution (e.g. make cleaned labels all becomes 0) if classification loss can be back-propagated to both Image Classifier and Label Cleaning Network.

## Results
![](https://i.imgur.com/tp0Vj6T.png)
