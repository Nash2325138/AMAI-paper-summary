# [Paper reading] Learning From Noisy Large-Scale Datasets With Minimal Supervision
Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie, CVPR 2017

## Introduction
Deep learning methods often require a large dataset to achieve good performance. However, datasets of this kind often require a huge amount of human annotations, which makes the collection process pretty expensive.

On the other hand, datasets with **unlabeled data or noisy annotations** are more common in reality, so lots of works related to unsupervied, semi-supervided, or self-supervided learning are studied recently to utilize the rich information under these datasets.


## Contribution

This work introduce an new approach to leverage datasets with a conjunction of **a small set of clean annotations and a large set of noisy annotations**.

One common approach is to pretrain a network on the noise data and then finetune the network with the clean dataset. This paper demonstrates that the proposed method significantly improved the performance over traditional fine-tuning approach and provide the first benchmark on the Open Image dataset.

## Problem Setting 
The method is experimented on a classic image classification problem. All of the image in the dataset has a noisy label $y$, but only a small portion of image have human-verified label $v$.

## Method

### Architecture
![](https://i.imgur.com/n6dmBGo.png)
The model architecture can be divided into 3 parts:
1. Convolutional Network: the image feature extractor
2. Label Cleaning Network: domain transformer from noisy label to clean label
3. Image Classifier: one fully connected layer followed by a sigmoid function

### General Concept
1. Use CNN to extract image features
2. Feed the image features along with noisy labels to "Label Cleaning Network" to predict a cleaned label $c$ and compute loss $L_{\text{clean}}$
3. Feed the image features to "Image Classifier" to predict a label $p$ and compute loss $L_{\text{classify}}$
4. Gradient decent

For $L_{\text{clean}}$, only the data samples **with verified labels** are used.

For $L_{\text{classify}}$, it's supervised by verified labels $v$ when available, otherwise the cleaned label $c$ will be used.

### Details

#### Losses Design
There two losses involved: $L_{\text{clean}}$ and $L_{\text{classify}}$

#### Archticture 

## Results

## Conclusion
