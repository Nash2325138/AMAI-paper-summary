# [Paper reading] Learning From Noisy Large-Scale Datasets With Minimal Supervision
Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie, CVPR 2017

## Introduction
Deep learning methods often require a large dataset to achieve good performance. However, datasets of this kind often require a huge amount of human annotations, which makes the collection process pretty expensive.

On the other hand, datasets with **unlabeled data or noisy annotations** are more common in reality, so lots of works related to unsupervied, semi-supervided, or self-supervided learning are studied recently to utilize the rich information under these datasets.


## Contribution

This work introduce an new approach to leverage datasets with a conjunction of **a small set of clean annotations and a large set of noisy annotations**.

One common approach is to pretrain a network on the noise data and then finetune the network with the clean dataset. This paper demonstrates that the proposed method significantly improved the performance over traditional fine-tuning approach and provide the first benchmark on the Open Image dataset.

## Method
![](https://i.imgur.com/n6dmBGo.png)



## Results

## Conclusion
